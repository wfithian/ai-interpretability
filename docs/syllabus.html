<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>AI Interpretability &amp; AI Risk</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">AI Interpretability &amp; AI Risk</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./syllabus.html" aria-current="page"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Schedule</h2>
   
  <ul>
  <li><a href="#group-information" id="toc-group-information" class="nav-link active" data-scroll-target="#group-information">Group Information</a></li>
  <li><a href="#about" id="toc-about" class="nav-link" data-scroll-target="#about">About</a></li>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link" data-scroll-target="#learning-objectives">Learning Objectives</a></li>
  <li><a href="#course-structure" id="toc-course-structure" class="nav-link" data-scroll-target="#course-structure">Course Structure</a></li>
  <li><a href="#part-i-foundations-orientation" id="toc-part-i-foundations-orientation" class="nav-link" data-scroll-target="#part-i-foundations-orientation">Part I: Foundations &amp; Orientation</a>
  <ul class="collapse">
  <li><a href="#week-1-framing-interpretability" id="toc-week-1-framing-interpretability" class="nav-link" data-scroll-target="#week-1-framing-interpretability">Week 1 — Framing Interpretability</a></li>
  <li><a href="#week-2-mechanistic-interpretability-and-circuits" id="toc-week-2-mechanistic-interpretability-and-circuits" class="nav-link" data-scroll-target="#week-2-mechanistic-interpretability-and-circuits">Week 2 — Mechanistic Interpretability and Circuits</a></li>
  <li><a href="#week-3-interpretability-as-a-safety-strategy" id="toc-week-3-interpretability-as-a-safety-strategy" class="nav-link" data-scroll-target="#week-3-interpretability-as-a-safety-strategy">Week 3 — Interpretability as a Safety Strategy</a></li>
  <li><a href="#week-4-causal-intervention-in-practice" id="toc-week-4-causal-intervention-in-practice" class="nav-link" data-scroll-target="#week-4-causal-intervention-in-practice">Week 4 — Causal Intervention in Practice</a></li>
  </ul></li>
  <li><a href="#part-ii-intervention-control-and-robustness" id="toc-part-ii-intervention-control-and-robustness" class="nav-link" data-scroll-target="#part-ii-intervention-control-and-robustness">Part II: Intervention, Control, and Robustness</a>
  <ul class="collapse">
  <li><a href="#week-5-steering-vectors" id="toc-week-5-steering-vectors" class="nav-link" data-scroll-target="#week-5-steering-vectors">Week 5 — Steering Vectors</a></li>
  <li><a href="#week-6-stability-of-steering" id="toc-week-6-stability-of-steering" class="nav-link" data-scroll-target="#week-6-stability-of-steering">Week 6 — Stability of Steering</a></li>
  <li><a href="#week-7-machine-unlearning" id="toc-week-7-machine-unlearning" class="nav-link" data-scroll-target="#week-7-machine-unlearning">Week 7 — Machine Unlearning</a></li>
  <li><a href="#week-8-latent-knowledge" id="toc-week-8-latent-knowledge" class="nav-link" data-scroll-target="#week-8-latent-knowledge">Week 8 — Latent Knowledge</a></li>
  <li><a href="#week-9-context-dependent-representations" id="toc-week-9-context-dependent-representations" class="nav-link" data-scroll-target="#week-9-context-dependent-representations">Week 9 — Context-Dependent Representations</a></li>
  <li><a href="#week-10-evaluating-interpretability-methods" id="toc-week-10-evaluating-interpretability-methods" class="nav-link" data-scroll-target="#week-10-evaluating-interpretability-methods">Week 10 — Evaluating Interpretability Methods</a></li>
  <li><a href="#week-11-causal-scrubbing" id="toc-week-11-causal-scrubbing" class="nav-link" data-scroll-target="#week-11-causal-scrubbing">Week 11 — Causal Scrubbing</a></li>
  </ul></li>
  <li><a href="#part-iii-failure-modes-explanation-and-alignment" id="toc-part-iii-failure-modes-explanation-and-alignment" class="nav-link" data-scroll-target="#part-iii-failure-modes-explanation-and-alignment">Part III: Failure Modes, Explanation, and Alignment</a>
  <ul class="collapse">
  <li><a href="#week-12-mesa-optimization-and-deceptive-alignment" id="toc-week-12-mesa-optimization-and-deceptive-alignment" class="nav-link" data-scroll-target="#week-12-mesa-optimization-and-deceptive-alignment">Week 12 — Mesa-Optimization and Deceptive Alignment</a></li>
  <li><a href="#week-13-interpretability-under-deception" id="toc-week-13-interpretability-under-deception" class="nav-link" data-scroll-target="#week-13-interpretability-under-deception">Week 13 — Interpretability Under Deception</a></li>
  <li><a href="#week-14-philosophy-of-causal-explanation" id="toc-week-14-philosophy-of-causal-explanation" class="nav-link" data-scroll-target="#week-14-philosophy-of-causal-explanation">Week 14 — Philosophy of Causal Explanation</a></li>
  <li><a href="#week-15-interpretability-and-existential-risk" id="toc-week-15-interpretability-and-existential-risk" class="nav-link" data-scroll-target="#week-15-interpretability-and-existential-risk">Week 15 — Interpretability and Existential Risk</a></li>
  </ul></li>
  <li><a href="#expectations-and-participation" id="toc-expectations-and-participation" class="nav-link" data-scroll-target="#expectations-and-participation">Expectations and Participation</a>
  <ul class="collapse">
  <li><a href="#preparation" id="toc-preparation" class="nav-link" data-scroll-target="#preparation">Preparation</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#presentations-optional" id="toc-presentations-optional" class="nav-link" data-scroll-target="#presentations-optional">Presentations (Optional)</a></li>
  <li><a href="#final-project-optional" id="toc-final-project-optional" class="nav-link" data-scroll-target="#final-project-optional">Final Project (Optional)</a></li>
  </ul></li>
  <li><a href="#academic-integrity" id="toc-academic-integrity" class="nav-link" data-scroll-target="#academic-integrity">Academic Integrity</a></li>
  <li><a href="#contact" id="toc-contact" class="nav-link" data-scroll-target="#contact">Contact</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">AI Interpretability &amp; AI Risk</h1>
<p class="subtitle lead">Graduate Reading Group — UC Berkeley — Spring 2025</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="group-information" class="level2">
<h2 class="anchored" data-anchor-id="group-information">Group Information</h2>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td><strong>Organizers</strong></td>
<td>Will Fithian &amp; Wes Holliday</td>
</tr>
<tr class="even">
<td><strong>Meeting Time</strong></td>
<td>[Day], [Time]</td>
</tr>
<tr class="odd">
<td><strong>Location</strong></td>
<td>[Building, Room Number]</td>
</tr>
</tbody>
</table>
</section>
<section id="about" class="level2">
<h2 class="anchored" data-anchor-id="about">About</h2>
<p>This reading group brings together researchers from multiple disciplines to develop a shared, rigorous understanding of AI interpretability—both as a scientific enterprise and as a potential tool for managing the risks posed by increasingly capable AI systems.</p>
<p>The group emphasizes <em>pragmatic mechanistic interpretability</em>: methods that make concrete, testable claims about how models work internally, and that plausibly contribute to oversight, control, and alignment.</p>
<p>Rather than surveying the literature broadly, the course is structured around <strong>one substantial reading per week</strong>, allowing participants to engage deeply with each work. The readings combine foundational papers that establish shared vocabulary with recent (2023–2025) frontier research on steering, model editing and unlearning, robustness under distribution shift, and the limits of interpretability in the presence of deception.</p>
</section>
<section id="learning-objectives" class="level2">
<h2 class="anchored" data-anchor-id="learning-objectives">Learning Objectives</h2>
<p>By the end of this course, participants will be able to:</p>
<ol type="1">
<li><strong>Identify</strong> what kind of explanation an interpretability paper offers</li>
<li><strong>Evaluate</strong> the robustness and validity of interpretability claims</li>
<li><strong>Understand</strong> how interpretability research connects to AI safety and alignment</li>
<li><strong>Recognize</strong> open problems and promising research directions</li>
<li><strong>Apply</strong> critical reasoning from multiple disciplines (ML, statistics, philosophy, causal inference) to interpretability research</li>
</ol>
</section>
<section id="course-structure" class="level2">
<h2 class="anchored" data-anchor-id="course-structure">Course Structure</h2>
<p>The course is organized into three sections reflecting a progression from <em>understanding</em> to <em>control</em> to <em>limits</em>:</p>
<ul>
<li><strong>Part I: Foundations &amp; Orientation</strong> (Weeks 1–4) — Conceptual grounding, mechanistic methods, and strategic goals</li>
<li><strong>Part II: Intervention, Control, and Robustness</strong> (Weeks 5–11) — What we can do to models and how to evaluate it</li>
<li><strong>Part III: Failure Modes, Explanation, and Alignment</strong> (Weeks 12–15) — When interpretability breaks and how it fits into AI risk</li>
</ul>
<hr>
</section>
<section id="part-i-foundations-orientation" class="level2">
<h2 class="anchored" data-anchor-id="part-i-foundations-orientation">Part I: Foundations &amp; Orientation</h2>
<section id="week-1-framing-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="week-1-framing-interpretability">Week 1 — Framing Interpretability</h3>
<dl>
<dt><strong>Towards a Rigorous Science of Interpretable Machine Learning</strong> (2017)</dt>
<dd>
Finale Doshi-Velez &amp; Been Kim
</dd>
<dd>
<a href="https://arxiv.org/abs/1702.08608" class="uri">https://arxiv.org/abs/1702.08608</a>
</dd>
</dl>
<p><em>Introduces a taxonomy of interpretability goals, audiences, and evaluation criteria; frames interpretability as an epistemic problem.</em></p>
</section>
<section id="week-2-mechanistic-interpretability-and-circuits" class="level3">
<h3 class="anchored" data-anchor-id="week-2-mechanistic-interpretability-and-circuits">Week 2 — Mechanistic Interpretability and Circuits</h3>
<dl>
<dt><strong>Zoom In: Circuits</strong> (2020)</dt>
<dd>
Chris Olah et al.
</dd>
<dd>
<a href="https://distill.pub/2020/circuits/" class="uri">https://distill.pub/2020/circuits/</a>
</dd>
</dl>
<p><em>Canonical introduction to mechanistic interpretability, circuit-based explanations, and feature-level analysis.</em></p>
</section>
<section id="week-3-interpretability-as-a-safety-strategy" class="level3">
<h3 class="anchored" data-anchor-id="week-3-interpretability-as-a-safety-strategy">Week 3 — Interpretability as a Safety Strategy</h3>
<p><strong>Neel Nanda</strong></p>
<ul>
<li><em>A Pragmatic Vision for Interpretability</em> <a href="https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability" class="uri">https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability</a></li>
<li><em>How Can Interpretability Researchers Help AGI Go Well?</em> <a href="https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well" class="uri">https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well</a></li>
</ul>
<p><em>Agenda-setting essays that articulate a pragmatic, safety-oriented theory of change for interpretability research.</em></p>
</section>
<section id="week-4-causal-intervention-in-practice" class="level3">
<h3 class="anchored" data-anchor-id="week-4-causal-intervention-in-practice">Week 4 — Causal Intervention in Practice</h3>
<dl>
<dt><strong>Locating and Editing Factual Associations in Transformer Language Models</strong> (2024)</dt>
<dd>
Kevin Meng et al.
</dd>
<dd>
<a href="https://arxiv.org/abs/2401.01967" class="uri">https://arxiv.org/abs/2401.01967</a>
</dd>
</dl>
<p><em>A modern example of causal and mechanistic intervention on real language models; bridges theory and practice.</em></p>
<hr>
</section>
</section>
<section id="part-ii-intervention-control-and-robustness" class="level2">
<h2 class="anchored" data-anchor-id="part-ii-intervention-control-and-robustness">Part II: Intervention, Control, and Robustness</h2>
<section id="week-5-steering-vectors" class="level3">
<h3 class="anchored" data-anchor-id="week-5-steering-vectors">Week 5 — Steering Vectors</h3>
<dl>
<dt><strong>Steering GPT-2-XL by Adding Directions in Activation Space</strong> (2023)</dt>
<dd>
Alexander Turner et al.
</dd>
<dd>
<a href="https://arxiv.org/abs/2303.11396" class="uri">https://arxiv.org/abs/2303.11396</a>
</dd>
</dl>
<p><em>Introduces steering vectors and linear control of model behavior via internal activations.</em></p>
</section>
<section id="week-6-stability-of-steering" class="level3">
<h3 class="anchored" data-anchor-id="week-6-stability-of-steering">Week 6 — Stability of Steering</h3>
<dl>
<dt><strong>Layer-Wise Stability of Steering Vectors</strong> (2024)</dt>
<dd>
Li et al.
</dd>
<dd>
<em>(Link to be finalized)</em>
</dd>
</dl>
<p><em>Analyzes how steering directions vary across layers, prompts, and contexts; central to robustness and persona sampling.</em></p>
</section>
<section id="week-7-machine-unlearning" class="level3">
<h3 class="anchored" data-anchor-id="week-7-machine-unlearning">Week 7 — Machine Unlearning</h3>
<dl>
<dt><strong>Unlearning Dangerous Capabilities in Language Models</strong> (2024)</dt>
<dd>
Vikram Maini et al.
</dd>
<dd>
<a href="https://arxiv.org/abs/2301.10728" class="uri">https://arxiv.org/abs/2301.10728</a>
</dd>
</dl>
<p><em>Explores selective forgetting and capability removal, raising deep questions about representation and control.</em></p>
</section>
<section id="week-8-latent-knowledge" class="level3">
<h3 class="anchored" data-anchor-id="week-8-latent-knowledge">Week 8 — Latent Knowledge</h3>
<dl>
<dt><strong>Discovering Latent Knowledge in Language Models Without Supervision</strong> (2023)</dt>
<dd>
Collin Burns et al.
</dd>
<dd>
<a href="https://arxiv.org/abs/2306.11644" class="uri">https://arxiv.org/abs/2306.11644</a>
</dd>
</dl>
<p><em>Separates what models know from what they say; foundational for oversight and evaluation.</em></p>
</section>
<section id="week-9-context-dependent-representations" class="level3">
<h3 class="anchored" data-anchor-id="week-9-context-dependent-representations">Week 9 — Context-Dependent Representations</h3>
<dl>
<dt><strong>Prompted Representations: Context-Dependent Embeddings and Hidden States</strong> (2024)</dt>
<dd>
Miller et al.
</dd>
<dd>
<em>(Link to be finalized)</em>
</dd>
</dl>
<p><em>Shows how prompts reshape internal representations, challenging assumptions of interpretability robustness.</em></p>
</section>
<section id="week-10-evaluating-interpretability-methods" class="level3">
<h3 class="anchored" data-anchor-id="week-10-evaluating-interpretability-methods">Week 10 — Evaluating Interpretability Methods</h3>
<dl>
<dt><strong>Evaluating Interpretability: A Framework for Benchmarking Methods</strong> (2024)</dt>
<dd>
Stephen Casper et al.
</dd>
<dd>
<a href="https://arxiv.org/abs/2303.16812" class="uri">https://arxiv.org/abs/2303.16812</a>
</dd>
</dl>
<p><em>Proposes criteria and benchmarks for determining whether interpretability methods actually work.</em></p>
</section>
<section id="week-11-causal-scrubbing" class="level3">
<h3 class="anchored" data-anchor-id="week-11-causal-scrubbing">Week 11 — Causal Scrubbing</h3>
<dl>
<dt><strong>Causal Scrubbing: Interventions for Mechanistic Interpretability</strong> (2023)</dt>
<dd>
Lawrence Chan et al.
</dd>
<dd>
<a href="https://arxiv.org/abs/2301.13163" class="uri">https://arxiv.org/abs/2301.13163</a>
</dd>
</dl>
<p><em>Introduces a disciplined framework for causal claims in mechanistic interpretability.</em></p>
<hr>
</section>
</section>
<section id="part-iii-failure-modes-explanation-and-alignment" class="level2">
<h2 class="anchored" data-anchor-id="part-iii-failure-modes-explanation-and-alignment">Part III: Failure Modes, Explanation, and Alignment</h2>
<section id="week-12-mesa-optimization-and-deceptive-alignment" class="level3">
<h3 class="anchored" data-anchor-id="week-12-mesa-optimization-and-deceptive-alignment">Week 12 — Mesa-Optimization and Deceptive Alignment</h3>
<dl>
<dt><strong>Risks from Learned Optimization in Advanced ML Systems</strong> (2019)</dt>
<dd>
Evan Hubinger et al.
</dd>
<dd>
<a href="https://arxiv.org/abs/1906.01820" class="uri">https://arxiv.org/abs/1906.01820</a>
</dd>
</dl>
<p><em>Classic account of mesa-optimization and deceptive alignment; essential for understanding interpretability’s limits.</em></p>
</section>
<section id="week-13-interpretability-under-deception" class="level3">
<h3 class="anchored" data-anchor-id="week-13-interpretability-under-deception">Week 13 — Interpretability Under Deception</h3>
<dl>
<dt><strong>Mechanistic Interpretability Can Fail in the Presence of Deception</strong> (2024)</dt>
<dd>
Barnett et al.
</dd>
<dd>
<em>(Link to be finalized)</em>
</dd>
</dl>
<p><em>Explores how strategically aware systems could evade or manipulate interpretability tools.</em></p>
</section>
<section id="week-14-philosophy-of-causal-explanation" class="level3">
<h3 class="anchored" data-anchor-id="week-14-philosophy-of-causal-explanation">Week 14 — Philosophy of Causal Explanation</h3>
<dl>
<dt><strong>Making Things Happen</strong> (2003), selected chapters</dt>
<dd>
James Woodward
</dd>
</dl>
<p><em>Philosophical grounding for causal explanation and intervention.</em></p>
</section>
<section id="week-15-interpretability-and-existential-risk" class="level3">
<h3 class="anchored" data-anchor-id="week-15-interpretability-and-existential-risk">Week 15 — Interpretability and Existential Risk</h3>
<dl>
<dt><strong>Is Power-Seeking AI an Existential Risk?</strong> (2022–2024), selected sections</dt>
<dd>
Joseph Carlsmith
</dd>
<dd>
<a href="https://arxiv.org/abs/2206.13353" class="uri">https://arxiv.org/abs/2206.13353</a>
</dd>
</dl>
<p><em>Situates interpretability within alignment, governance, and long-term AI risk.</em></p>
<hr>
</section>
</section>
<section id="expectations-and-participation" class="level2">
<h2 class="anchored" data-anchor-id="expectations-and-participation">Expectations and Participation</h2>
<section id="preparation" class="level3">
<h3 class="anchored" data-anchor-id="preparation">Preparation</h3>
<ul>
<li>Complete the assigned reading before each session</li>
<li>Come prepared with questions, critiques, or connections to other work</li>
</ul>
</section>
<section id="discussion" class="level3">
<h3 class="anchored" data-anchor-id="discussion">Discussion</h3>
<ul>
<li>Active participation is expected; the value of the group depends on collective engagement</li>
<li>Discussions will focus on: What does this paper claim? How do we evaluate those claims? What are the implications for AI safety?</li>
</ul>
</section>
<section id="presentations-optional" class="level3">
<h3 class="anchored" data-anchor-id="presentations-optional">Presentations (Optional)</h3>
<p>[Each participant will lead discussion for X session(s) during the semester.]</p>
</section>
<section id="final-project-optional" class="level3">
<h3 class="anchored" data-anchor-id="final-project-optional">Final Project (Optional)</h3>
<p>[Description of final project requirements, if applicable.]</p>
</section>
</section>
<section id="academic-integrity" class="level2">
<h2 class="anchored" data-anchor-id="academic-integrity">Academic Integrity</h2>
<p>All participants are expected to uphold the highest standards of academic integrity. When presenting others’ ideas, proper attribution is required.</p>
</section>
<section id="contact" class="level2">
<h2 class="anchored" data-anchor-id="contact">Contact</h2>
<p>Questions? Contact Will Fithian (<a href="mailto:wfithian@berkeley.edu">wfithian@berkeley.edu</a>) or Wes Holliday (<a href="mailto:wesholliday@berkeley.edu">wesholliday@berkeley.edu</a>).</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>