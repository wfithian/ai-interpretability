---
title: "AI Interpretability: Meaning, Methods, and Limitations"
subtitle: "Graduate Reading Group — UC Berkeley — Spring 2026"
format:
  html:
    theme: cosmo
    toc: true
    toc-location: left
    toc-title: "Contents"
    toc-depth: 3
    page-layout: article
    css: styles.css
---

An interdisciplinary reading group on AI interpretability and its connection to AI safety. We read **one substantial paper per week** to enable deep engagement rather than surface coverage.

**Organizers:** Will Fithian & Wes Holliday

::: {.callout-note}
## Tentative Reading List
The reading list below is tentative and may be revised before the semester begins.
:::

## Reading Schedule

The readings progress from *understanding* to *control* to *limits*.

### Part I: Foundations & Orientation (Weeks 1–4)

| Week | Topic | Reading |
|------|-------|---------|
| 1 | Framing Interpretability | [Towards a Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608) — Doshi-Velez & Kim (2017) |
| 2 | Mechanistic Interpretability | [Zoom In: Circuits](https://distill.pub/2020/circuits/) — Olah et al. (2020) |
| 3 | Interpretability as Safety Strategy | [A Pragmatic Vision for Interpretability](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability) + [How Can Interpretability Researchers Help AGI Go Well?](https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well) — Nanda (2025) |
| 4 | Causal Intervention (ROME) | [Locating and Editing Factual Associations in GPT](https://arxiv.org/abs/2202.05262) — Meng et al. (NeurIPS 2022) |

### Part II: Intervention, Control, and Robustness (Weeks 5–10)

| Week | Topic | Reading |
|------|-------|---------|
| 5 | Steering Vectors | [Steering Language Models With Activation Engineering](https://arxiv.org/abs/2308.10248) — Turner et al. (2023) |
| 6 | Behavioral Directions | [Refusal in LLMs Is Mediated by a Single Direction](https://arxiv.org/abs/2402.07896) — Arditi et al. (2024) + [Persona Vectors](https://www.anthropic.com/research/persona-vectors) — Anthropic (2025) |
| 7 | Machine Unlearning | [TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121) — Maini et al. (2024) |
| 8 | Latent Knowledge | [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827) — Burns et al. (ICLR 2023) |
| 9 | Causal Scrubbing | [Causal Scrubbing: Rigorously Testing Mechanistic Hypotheses](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing) — Chan et al. (2022) |
| 10 | Large-Scale Mechanistic Interp | [On the Biology of a Large Language Model](https://www.anthropic.com/research/biology-of-a-large-language-model) — Anthropic (2025) |

### Part III: Failure Modes, Explanation, and Alignment (Weeks 11–15)

| Week | Topic | Reading |
|------|-------|---------|
| 11 | Mesa-Optimization | [Risks from Learned Optimization in Advanced ML Systems](https://arxiv.org/abs/1906.01820) — Hubinger et al. (2019) |
| 12 | Deceptive Interpretability | [Deceptive Automated Interpretability: LMs Coordinating to Fool Oversight](https://arxiv.org/abs/2504.07831) — Lermen et al. (2025) |
| 13 | Philosophy of Causal Explanation | *Making Things Happen*, selected chapters — Woodward (2003) |
| 14 | Causal Abstractions | [Causal Abstractions of Neural Networks](https://arxiv.org/abs/2303.02536) — Geiger et al. (2023) |
| 15 | Interpretability and Existential Risk | [Is Power-Seeking AI an Existential Risk?](https://arxiv.org/abs/2206.13353), selected sections — Carlsmith (2022–2024) |

See the [full syllabus](syllabus.qmd) for paper summaries and additional context.

---

## About the Group

This reading group brings together researchers from Statistics, Computer Science, Philosophy, Neuroscience, and related fields to develop a shared understanding of AI interpretability—both as a scientific enterprise and as a potential tool for managing the risks posed by increasingly capable AI systems.

We emphasize *pragmatic mechanistic interpretability*: methods that make concrete, testable claims about how models work internally, and that plausibly contribute to oversight, control, and alignment.

### What We're Looking For

- Comfort reading technical ML papers
- Interest in evaluating interpretability claims critically
- Perspectives from statistics, philosophy, causal inference, or cognitive science welcome

No prior interpretability research experience required.

## Meeting Details

| | |
|---|---|
| **Time** | TBD |
| **Location** | TBD |
| **First Meeting** | TBD |
| **Units** | 1–2 |

## Contact

Questions? Contact Will Fithian ([wfithian@berkeley.edu](mailto:wfithian@berkeley.edu)) or Wes Holliday ([wesholliday@berkeley.edu](mailto:wesholliday@berkeley.edu)).
