---
title: "AI Interpretability: Meaning, Methods, and Limitations"
subtitle: "Graduate Reading Group — UC Berkeley — Spring 2026"
format:
  html:
    theme: cosmo
    toc: true
    toc-location: left
    toc-title: "Contents"
    toc-depth: 3
    page-layout: article
    css: styles.css
---

An interdisciplinary reading group organized around a single question:

> **What kinds of interpretability claims are meaningful, reliable, and decision-relevant as AI systems become more capable, opaque, and strategically situated?**

We read **one substantial paper per week** to enable deep engagement rather than surface coverage.

**Organizers:** Will Fithian & Wes Holliday

::: {.callout-tip}
## Interested in joining?
**[Fill out this form to indicate interest](https://forms.gle/WBqohweBJutZSQ7b7)** — we'll follow up with details as the semester approaches.
:::

::: {.callout-note}
## Tentative Reading List
The reading list below is tentative and may be revised before the semester begins.
:::

## Reading Schedule

The course follows the arc of its title: **Meaning → Methods → Limitations**

### Part I: Meaning and Orientation (Weeks 1–4)

*What is interpretability for? What kinds of questions can it answer? How do mechanistic explanations differ from post-hoc rationalizations?*

| Week | Topic | Reading |
|------|-------|---------|
| 1 | Framing Interpretability | [Towards a Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608) — Doshi-Velez & Kim (2017) |
| 2 | Mechanistic Interpretability | [Zoom In: Circuits](https://distill.pub/2020/circuits/) — Olah et al. (2020) |
| 3 | Interpretability as Safety Strategy | [A Pragmatic Vision for Interpretability](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability) + [How Can Interpretability Researchers Help AGI Go Well?](https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well) — Nanda (2025) |
| 4 | Causal Intervention (ROME) | [Locating and Editing Factual Associations in GPT](https://arxiv.org/abs/2202.05262) — Meng et al. (NeurIPS 2022) |

### Part II: Methods, Interventions, and Internal Structure (Weeks 5–10)

*What can interpretability methods actually do with real models? When does an intervention support an explanatory claim, and when is it merely a control knob?*

| Week | Topic | Reading |
|------|-------|---------|
| 5 | Steering Vectors | [Steering Language Models With Activation Engineering](https://arxiv.org/abs/2308.10248) — Turner et al. (2023) |
| 6 | Behavioral Directions | [Refusal in LLMs Is Mediated by a Single Direction](https://arxiv.org/abs/2402.07896) — Arditi et al. (2024) + [Persona Vectors](https://www.anthropic.com/research/persona-vectors) — Anthropic (2025) |
| 7 | Machine Unlearning | [TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121) — Maini et al. (2024) |
| 8 | Latent Knowledge | [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827) — Burns et al. (ICLR 2023) |
| 9 | Causal Scrubbing | [Causal Scrubbing: Rigorously Testing Mechanistic Hypotheses](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing) — Chan et al. (2022) |
| 10 | Large-Scale Mechanistic Interp | [On the Biology of a Large Language Model](https://www.anthropic.com/research/biology-of-a-large-language-model) — Anthropic (2025) |

### Part III: Limits, Evaluation, and Stakes (Weeks 11–15)

*When does interpretability evidence support counterfactual reasoning? Given its limits, what role should interpretability play in reducing catastrophic risk?*

| Week | Topic | Reading |
|------|-------|---------|
| 11 | Mesa-Optimization | [Risks from Learned Optimization in Advanced ML Systems](https://arxiv.org/abs/1906.01820) — Hubinger et al. (2019) |
| 12 | Deceptive Interpretability | [Deceptive Automated Interpretability: LMs Coordinating to Fool Oversight](https://arxiv.org/abs/2504.07831) — Lermen et al. (2025) |
| 13 | Philosophy of Causal Explanation | *Making Things Happen*, selected chapters — Woodward (2003) |
| 14 | Causal Abstractions | [Causal Abstractions of Neural Networks](https://arxiv.org/abs/2303.02536) — Geiger et al. (2023) |
| 15 | Interpretability and Existential Risk | [Is Power-Seeking AI an Existential Risk?](https://arxiv.org/abs/2206.13353), selected sections — Carlsmith (2022–2024) |

See the [full syllabus](syllabus.qmd) for paper summaries and additional context.

---

## About the Group

This reading group treats interpretability as a *scientific and safety-relevant practice*: a collection of methods, epistemic standards, and theory-of-change assumptions aimed at gaining traction on the internal structure of modern ML systems in ways that could plausibly matter for oversight, control, and alignment.

The group is intentionally interdisciplinary, bringing together perspectives from machine learning, statistics, causal inference, philosophy of science, and AI governance. A core goal is to establish **shared evaluative standards** across these communities—standards for what counts as evidence, explanation, robustness, and limitation in interpretability research.

### What We're Looking For

- Comfort reading technical ML papers
- Interest in evaluating interpretability claims critically
- Perspectives from statistics, philosophy, causal inference, or cognitive science welcome

No prior interpretability research experience required.

## Meeting Details

TBD

## Contact

Questions? Contact Will Fithian ([wfithian@berkeley.edu](mailto:wfithian@berkeley.edu)) or Wes Holliday ([wesholliday@berkeley.edu](mailto:wesholliday@berkeley.edu)).
