---
title: "AI Interpretability: Meaning, Methods, and Limitations"
subtitle: "Graduate Reading Group — UC Berkeley — Spring 2026"
format:
  html:
    theme: cosmo
    toc: true
    toc-location: left
    toc-title: "Contents"
    toc-depth: 3
    page-layout: article
    css: styles.css
---

## Description

This is an interdisciplinary reading group organized around the question:
> **What kinds of interpretability claims are meaningful and reliable as AI systems become more capable and are deployed in higher-stakes contexts?**

We will study interpretability as a *scientific and safety-relevant practice*: a collection of methods, epistemic standards, and theory-of-change assumptions aimed at gaining traction on the internal structure of modern ML systems in ways that could plausibly matter for oversight, control, and alignment. Questions include:

- Why study interpretability? How do we formulate interpretability claims rigorously, and assess evidence for and against?
- How are interpretability methods currently used to understand and steer state-of-the-art systems? How do we know if they are working?
- What are the limitations of these methods, or of the interpretability paradigm more broadly? What role can it play in reducing risks to society?

Our aim is to bring together an interdisciplinary community including perspectives from machine learning, statistics, causal inference, philosophy of science, and AI governance, and to seed new research programs.

### Learning objectives
By the end of this course, participants should be able to:

1. **Distinguish** different senses of "interpretability" and "understanding"
2. **Evaluate** mechanistic claims in light of causal and robustness criteria
3. **Recognize** when interpretability evidence is strong, weak, or misleading
4. **Articulate** realistic theories of change connecting interpretability to AI risk mitigation

### Assumed background

We welcome broad interdisciplinary participation, but our curriculum is designed for participants with:
- Comfort reading technical ML papers
- Interest in evaluating interpretability claims critically
- Background in one or more relevant fields such as statistics, philosophy, causal inference, or cognitive science

No prior interpretability research experience is required.

**Interested? [RSVP here](https://forms.gle/WBqohweBJutZSQ7b7)**

## Logistics

**Format:** We will read 1--2 papers per week; one group member will lead a 45-minute presentation followed by group discussion.

**Organizers:** Will Fithian & Wes Holliday

**Meeting Details:** TBD

**Course credit:** We can enroll a limited number of students for up to 2 units of course credit. The requirement for course credit is regular attendance plus a project submitted during RRR week. Please indicate your interest via the [RSVP form](https://forms.gle/WBqohweBJutZSQ7b7).



## Tentative Reading Schedule

The reading list below may be revised according to participant interests.

### Part I: Meaning and Orientation (Weeks 1–3)

*What is interpretability for? What kinds of questions can it answer? How do mechanistic explanations differ from post-hoc rationalizations?*

| Week | Topic | Reading |
|------|-------|---------|
| 1 | Framing Interpretability | [Towards a Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608) — Doshi-Velez & Kim (2017) |
| 2 | Mechanistic Interpretability | [Zoom In: Circuits](https://distill.pub/2020/circuits/) — Olah et al. (2020) |
| 3 | Interpretability as Safety Strategy | [A Pragmatic Vision for Interpretability](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability) + [How Can Interpretability Researchers Help AGI Go Well?](https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well) — Nanda (2025) |

### Part II: Methods, Interventions, and Internal Structure (Weeks 4–10)

*What can interpretability methods actually do with real models? When does an intervention support an explanatory claim, and when is it merely a control knob?*

| Week | Topic | Reading |
|------|-------|---------|
| 4 | Causal Intervention (ROME) | [Locating and Editing Factual Associations in GPT](https://arxiv.org/abs/2202.05262) — Meng et al. (NeurIPS 2022) |
| 5 | Steering Vectors | [Steering Language Models With Activation Engineering](https://arxiv.org/abs/2308.10248) — Turner et al. (2023) |
| 6 | Behavioral Directions | [Refusal in LLMs Is Mediated by a Single Direction](https://arxiv.org/abs/2402.07896) — Arditi et al. (2024) + [Persona Vectors](https://www.anthropic.com/research/persona-vectors) — Anthropic (2025) |
| 7 | Machine Unlearning | [TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121) — Maini et al. (2024) |
| 8 | Latent Knowledge | [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827) — Burns et al. (ICLR 2023) |
| 9 | Causal Scrubbing | [Causal Scrubbing: Rigorously Testing Mechanistic Hypotheses](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing) — Chan et al. (2022) |
| 10 | Large-Scale Mechanistic Interp | [On the Biology of a Large Language Model](https://www.anthropic.com/research/biology-of-a-large-language-model) — Anthropic (2025) |

### Part III: Limits, Evaluation, and Stakes (Weeks 11–15)

*When does interpretability evidence support counterfactual reasoning? Given its limits, what role can interpretability play in reducing catastrophic risk?*

| Week | Topic | Reading |
|------|-------|---------|
| 11 | Mesa-Optimization | [Risks from Learned Optimization in Advanced ML Systems](https://arxiv.org/abs/1906.01820) — Hubinger et al. (2019) |
| 12 | Deceptive Interpretability | [Deceptive Automated Interpretability: LMs Coordinating to Fool Oversight](https://arxiv.org/abs/2504.07831) — Lermen et al. (2025) |
| 13 | Philosophy of Causal Explanation | *Making Things Happen*, selected chapters — Woodward (2003) |
| 14 | Causal Abstractions | [Causal Abstractions of Neural Networks](https://arxiv.org/abs/2303.02536) — Geiger et al. (2023) |
| 15 | Interpretability and Existential Risk | [Is Power-Seeking AI an Existential Risk?](https://arxiv.org/abs/2206.13353), selected sections — Carlsmith (2022–2024) |

See the [full syllabus](syllabus.qmd) for paper summaries and additional context.

---


## Contact

Questions? Contact Will Fithian ([wfithian@berkeley.edu](mailto:wfithian@berkeley.edu)) or Wes Holliday ([wesholliday@berkeley.edu](mailto:wesholliday@berkeley.edu)).
