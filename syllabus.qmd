---
title: "AI Interpretability: Meaning, Methods, and Limitations"
subtitle: "Graduate Reading Group — UC Berkeley — Spring 2026"
format:
  html:
    theme: cosmo
    toc: true
    toc-location: left
    toc-title: "Schedule"
    toc-depth: 3
    page-layout: article
    css: styles.css
---

## Group Information

**Organizers:** Will Fithian & Wes Holliday

**Meeting details:** TBD

**Units:** 1–2

::: {.callout-note}
## Tentative Reading List
The reading list below is tentative and may be revised before the semester begins.
:::

## About

This reading group is organized around a single, demanding question:

> **What kinds of interpretability claims are meaningful, reliable, and decision-relevant as AI systems become more capable, opaque, and strategically situated?**

The course is not a general survey of explainable AI, nor an attempt to catalog interpretability techniques. Instead, it treats interpretability as a *scientific and safety-relevant practice*: a collection of methods, epistemic standards, and theory-of-change assumptions aimed at gaining traction on the internal structure of modern ML systems in ways that could plausibly matter for oversight, control, and alignment.

The reading group is intentionally interdisciplinary, bringing together perspectives from machine learning, statistics, causal inference, philosophy of science, cognitive science, and AI governance. A core goal is to establish **shared evaluative standards** across these communities—standards for what counts as evidence, explanation, robustness, and limitation in interpretability research.

## Learning Objectives

By the end of this course, participants should be able to:

1. **Distinguish** different senses of "interpretability" and "understanding"
2. **Evaluate** mechanistic claims in light of causal and robustness criteria
3. **Recognize** when interpretability evidence is strong, weak, or misleading
4. **Articulate** realistic theories of change connecting interpretability to AI risk mitigation

The aim is not consensus, but **shared standards of argument**.

## Course Structure

The course follows the arc of its title: **Meaning → Methods → Limitations**

Rather than beginning with philosophical abstraction or ending with technical optimism, the syllabus is designed so that participants *earn* their skepticism and *discipline* their ambitions through sustained engagement with concrete research.

- **Part I: Meaning and Orientation** (Weeks 1–4) — What is interpretability for? What kinds of questions can it answer?
- **Part II: Methods, Interventions, and Internal Structure** (Weeks 5–10) — What can we actually do with real models?
- **Part III: Limits, Evaluation, and Stakes** (Weeks 11–15) — When does interpretability work, and what role should it play in AI risk?

---

## Part I: Meaning and Orientation

*The opening weeks establish what interpretability is for, what kinds of questions it can reasonably be expected to answer, and how mechanistic explanations differ from post-hoc rationalizations.*

### Week 1 — Framing Interpretability

**Towards a Rigorous Science of Interpretable Machine Learning** (2017)
: Finale Doshi-Velez & Been Kim
: <https://arxiv.org/abs/1702.08608>

*Introduces a taxonomy of interpretability goals, audiences, and evaluation criteria; frames interpretability as an epistemic problem.*

### Week 2 — Mechanistic Interpretability and Circuits

**Zoom In: Circuits** (2020)
: Chris Olah et al.
: <https://distill.pub/2020/circuits/>

*Canonical introduction to mechanistic interpretability, circuit-based explanations, and feature-level analysis.*

### Week 3 — Interpretability as a Safety Strategy

**Neel Nanda** (2025)

- *A Pragmatic Vision for Interpretability*
  <https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability>
- *How Can Interpretability Researchers Help AGI Go Well?*
  <https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well>

*Agenda-setting essays that articulate a pragmatic, safety-oriented theory of change for interpretability research.*

### Week 4 — Causal Intervention in Practice

**Locating and Editing Factual Associations in GPT** (NeurIPS 2022)
: Kevin Meng et al.
: <https://arxiv.org/abs/2202.05262>
: Project page: <https://rome.baulab.info/>

*ROME: causal tracing to identify where factual associations live in transformer computation, and targeted weight editing to modify them.*

---

## Part II: Methods, Interventions, and Internal Structure

*The middle of the course focuses on what interpretability methods actually allow us to do with real models. The emphasis is on intervention-oriented mechanistic work. Throughout this section, participants are encouraged to track an implicit question: When does an intervention support an explanatory claim, and when is it merely a control knob?*

### Week 5 — Steering Vectors

**Steering Language Models With Activation Engineering** (2023)
: Alexander Matt Turner et al.
: <https://arxiv.org/abs/2308.10248>

*Activation Addition (ActAdd): inference-time steering by adding activation-space directions computed from prompt pairs; foundational steering method.*

### Week 6 — Behavioral Directions and Internal State Variables

**Refusal in Language Models Is Mediated by a Single Direction** (2024)
: Yoni Arditi, Neel Nanda et al.
: <https://arxiv.org/abs/2402.07896>

**Persona Vectors: Monitoring and Steering Model Behavior** (2025)
: Anthropic Interpretability Team
: <https://www.anthropic.com/research/persona-vectors>

*Together, these readings show that important model behaviors can be mediated by low-dimensional structure in activation space. The refusal paper provides a crisp mechanistic result for a single high-stakes behavior, while persona vectors generalize this idea to broader, more persistent behavioral traits and recast interpretability as monitoring and oversight infrastructure. This week sets up later discussions about robustness, abstraction, and the limits of behavioral interpretability.*

### Week 7 — Machine Unlearning

**TOFU: A Task of Fictitious Unlearning for LLMs** (2024)
: Pratyush Maini et al.
: <https://arxiv.org/abs/2401.06121>

*Benchmark + evaluation suite for LLM unlearning; clarifies what it would mean to behave as if certain data were never learned.*

### Week 8 — Latent Knowledge

**Discovering Latent Knowledge in Language Models Without Supervision** (ICLR 2023; arXiv 2022)
: Collin Burns et al.
: <https://arxiv.org/abs/2212.03827>

*Extracts truth-related structure from activations to separate what models know from what they say; foundational for oversight.*

### Week 9 — Causal Scrubbing

**Causal Scrubbing: A Method for Rigorously Testing Mechanistic Interpretability Hypotheses** (2022)
: Lawrence Chan et al.
: <https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing>

*Behavior-preserving resampling ablations as a principled way to test mechanistic hypotheses (interpretation → falsifiable causal claim).*

### Week 10 — Large-Scale Mechanistic Interpretability

**On the Biology of a Large Language Model** (2025)
: Anthropic Interpretability Team
: <https://www.anthropic.com/research/biology-of-a-large-language-model>

*Presents Anthropic's most ambitious attempt to construct detailed mechanistic explanations of a frontier language model, tracing behavior through features, circuits, and attribution graphs. This paper serves as a flagship example of large-scale mechanistic interpretability in practice. Read in light of causal scrubbing, this week focuses on evaluating which parts of the explanatory narrative are causally supported, which rely on weaker forms of evidence, and what standards should govern claims of "understanding" at scale.*

---

## Part III: Limits, Evaluation, and Stakes

*The final section reframes interpretability from a technical toolkit into a contested epistemic strategy. By this point, participants have seen genuine successes, ambiguous interventions, and the temptation to compress complex evidence into persuasive narratives. Part III asks what follows from that experience.*

### Week 11 — Mesa-Optimization and Deceptive Alignment

**Risks from Learned Optimization in Advanced ML Systems** (2019)
: Evan Hubinger et al.
: <https://arxiv.org/abs/1906.01820>

*Classic account of mesa-optimization and deceptive alignment; essential for understanding interpretability's limits.*

### Week 12 — Deceptive Interpretability

**Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems** (2025)
: Simon Lermen, Mateusz Dziemian, Natalia Pérez-Campanero Antolín
: <https://arxiv.org/abs/2504.07831>

*Shows how models/agents can produce deceptive explanations that evade automated interpretability-based oversight; stress-tests interpretability as safety infrastructure.*

### Week 13 — Philosophy of Causal Explanation

**Making Things Happen** (2003), selected chapters
: James Woodward

*Philosophical grounding for causal explanation and intervention.*

### Week 14 — Causal Abstractions

**Causal Abstractions of Neural Networks** (2023)
: Atticus Geiger, DeepMind et al.
: <https://arxiv.org/abs/2303.02536>

*Asks whether mechanistic interpretability explanations support valid higher-level causal abstractions. Develops formal criteria for abstraction-preserving interventions and shows that many mechanistic stories fail to license the explanations they appear to offer. Serves as a rigorous stress test for interpretability claims in light of interventionist philosophy.*

### Week 15 — Interpretability and Existential Risk

**Is Power-Seeking AI an Existential Risk?** (2022–2024), selected sections
: Joseph Carlsmith
: <https://arxiv.org/abs/2206.13353>

*Situates interpretability within alignment, governance, and long-term AI risk.*

---

## Expectations and Participation

### Preparation
- Complete the assigned reading before each session
- Come prepared with questions, critiques, or connections to other work

### Discussion
- Active participation is expected; the value of the group depends on collective engagement
- Discussions will focus on: What does this paper claim? How do we evaluate those claims? What are the implications for AI safety?

### Presentations (Optional)
[Each participant will lead discussion for X session(s) during the semester.]

### Final Project (Optional)
[Description of final project requirements, if applicable.]

## Academic Integrity

All participants are expected to uphold the highest standards of academic integrity. When presenting others' ideas, proper attribution is required.

## Contact

Questions? Contact Will Fithian ([wfithian@berkeley.edu](mailto:wfithian@berkeley.edu)) or Wes Holliday ([wesholliday@berkeley.edu](mailto:wesholliday@berkeley.edu)).
