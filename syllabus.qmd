---
title: "AI Interpretability & AI Risk"
subtitle: "Graduate Reading Group — UC Berkeley — Spring 2026"
format:
  html:
    theme: cosmo
    toc: true
    toc-location: left
    toc-title: "Schedule"
    toc-depth: 3
    page-layout: article
    css: styles.css
---

## Group Information

| | |
|---|---|
| **Organizers** | Will Fithian & Wes Holliday |
| **Meeting Time** | [Day], [Time] |
| **Location** | [Building, Room Number] |

## About

This reading group brings together researchers from multiple disciplines to develop a shared, rigorous understanding of AI interpretability—both as a scientific enterprise and as a potential tool for managing the risks posed by increasingly capable AI systems.

The group emphasizes *pragmatic mechanistic interpretability*: methods that make concrete, testable claims about how models work internally, and that plausibly contribute to oversight, control, and alignment.

Rather than surveying the literature broadly, the course is structured around **one substantial reading per week**, allowing participants to engage deeply with each work. The readings combine foundational papers that establish shared vocabulary with recent (2023–2025) frontier research on steering, model editing and unlearning, robustness under distribution shift, and the limits of interpretability in the presence of deception.

## Learning Objectives

By the end of this course, participants will be able to:

1. **Identify** what kind of explanation an interpretability paper offers
2. **Evaluate** the robustness and validity of interpretability claims
3. **Understand** how interpretability research connects to AI safety and alignment
4. **Recognize** open problems and promising research directions
5. **Apply** critical reasoning from multiple disciplines (ML, statistics, philosophy, causal inference) to interpretability research

## Course Structure

The course is organized into three sections reflecting a progression from *understanding* to *control* to *limits*:

- **Part I: Foundations & Orientation** (Weeks 1–4) — Conceptual grounding, mechanistic methods, and strategic goals
- **Part II: Intervention, Control, and Robustness** (Weeks 5–11) — What we can do to models and how to evaluate it
- **Part III: Failure Modes, Explanation, and Alignment** (Weeks 12–15) — When interpretability breaks and how it fits into AI risk

---

## Part I: Foundations & Orientation

### Week 1 — Framing Interpretability

**Towards a Rigorous Science of Interpretable Machine Learning** (2017)
: Finale Doshi-Velez & Been Kim
: <https://arxiv.org/abs/1702.08608>

*Introduces a taxonomy of interpretability goals, audiences, and evaluation criteria; frames interpretability as an epistemic problem.*

### Week 2 — Mechanistic Interpretability and Circuits

**Zoom In: Circuits** (2020)
: Chris Olah et al.
: <https://distill.pub/2020/circuits/>

*Canonical introduction to mechanistic interpretability, circuit-based explanations, and feature-level analysis.*

### Week 3 — Interpretability as a Safety Strategy

**Neel Nanda** (2025)

- *A Pragmatic Vision for Interpretability*
  <https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability>
- *How Can Interpretability Researchers Help AGI Go Well?*
  <https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well>

*Agenda-setting essays that articulate a pragmatic, safety-oriented theory of change for interpretability research.*

### Week 4 — Causal Intervention in Practice

**Locating and Editing Factual Associations in GPT** (NeurIPS 2022)
: Kevin Meng et al.
: <https://arxiv.org/abs/2202.05262>
: Project page: <https://rome.baulab.info/>

*ROME: causal tracing to identify where factual associations live in transformer computation, and targeted weight editing to modify them.*

---

## Part II: Intervention, Control, and Robustness

### Week 5 — Steering Vectors

**Steering Language Models With Activation Engineering** (2023)
: Alexander Matt Turner et al.
: <https://arxiv.org/abs/2308.10248>

*Activation Addition (ActAdd): inference-time steering by adding activation-space directions computed from prompt pairs; foundational steering method.*

### Week 6 — Angular Steering

**Angular Steering: Behavior Control via Rotation in Activation Space** (2025)
: Hieu M. Vu & Tan M. Nguyen
: <https://arxiv.org/abs/2510.26243>

*Frames steering as rotation of activations toward/away from a target direction; good for discussing stability/selectivity and collateral effects.*

### Week 7 — Machine Unlearning

**TOFU: A Task of Fictitious Unlearning for LLMs** (2024)
: Pratyush Maini et al.
: <https://arxiv.org/abs/2401.06121>

*Benchmark + evaluation suite for LLM unlearning; clarifies what it would mean to behave as if certain data were never learned.*

### Week 8 — Latent Knowledge

**Discovering Latent Knowledge in Language Models Without Supervision** (ICLR 2023; arXiv 2022)
: Collin Burns et al.
: <https://arxiv.org/abs/2212.03827>

*Extracts truth-related structure from activations to separate what models know from what they say; foundational for oversight.*

### Week 9 — Prompt Sensitivity

**On the Worst Prompt Performance of Large Language Models** (2024)
: Bowen Cao et al.
: <https://arxiv.org/abs/2406.10248>

*Quantifies prompt sensitivity via "worst prompt" evaluation; forces a discussion of context distribution shift and fragility of conclusions.*

### Week 10 — Evaluating Interpretability Methods

**Benchmarking Interpretability Tools for Deep Neural Networks** (2023)
: Stephen Casper et al.
: <https://arxiv.org/abs/2302.10894>

*Interpretability needs empirical benchmarks; proposes trojan rediscovery as a ground-truth testbed for comparing tools.*

### Week 11 — Causal Scrubbing

**Causal Scrubbing: A Method for Rigorously Testing Mechanistic Interpretability Hypotheses** (2022)
: Lawrence Chan et al.
: <https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing>

*Behavior-preserving resampling ablations as a principled way to test mechanistic hypotheses (interpretation → falsifiable causal claim).*

---

## Part III: Failure Modes, Explanation, and Alignment

### Week 12 — Mesa-Optimization and Deceptive Alignment

**Risks from Learned Optimization in Advanced ML Systems** (2019)
: Evan Hubinger et al.
: <https://arxiv.org/abs/1906.01820>

*Classic account of mesa-optimization and deceptive alignment; essential for understanding interpretability's limits.*

### Week 13 — Deceptive Interpretability

**Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems** (2025)
: Simon Lermen, Mateusz Dziemian, Natalia Pérez-Campanero Antolín
: <https://arxiv.org/abs/2504.07831>

*Shows how models/agents can produce deceptive explanations that evade automated interpretability-based oversight; stress-tests interpretability as safety infrastructure.*

### Week 14 — Philosophy of Causal Explanation

**Making Things Happen** (2003), selected chapters
: James Woodward

*Philosophical grounding for causal explanation and intervention.*

### Week 15 — Interpretability and Existential Risk

**Is Power-Seeking AI an Existential Risk?** (2022–2024), selected sections
: Joseph Carlsmith
: <https://arxiv.org/abs/2206.13353>

*Situates interpretability within alignment, governance, and long-term AI risk.*

---

## Expectations and Participation

### Preparation
- Complete the assigned reading before each session
- Come prepared with questions, critiques, or connections to other work

### Discussion
- Active participation is expected; the value of the group depends on collective engagement
- Discussions will focus on: What does this paper claim? How do we evaluate those claims? What are the implications for AI safety?

### Presentations (Optional)
[Each participant will lead discussion for X session(s) during the semester.]

### Final Project (Optional)
[Description of final project requirements, if applicable.]

## Academic Integrity

All participants are expected to uphold the highest standards of academic integrity. When presenting others' ideas, proper attribution is required.

## Contact

Questions? Contact Will Fithian ([wfithian@berkeley.edu](mailto:wfithian@berkeley.edu)) or Wes Holliday ([wesholliday@berkeley.edu](mailto:wesholliday@berkeley.edu)).
