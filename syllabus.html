<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>AI Interpretability: Meaning, Methods, and Limitations – AI Interpretability</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">AI Interpretability</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./syllabus.html" aria-current="page"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Schedule</h2>
   
  <ul>
  <li><a href="#curriculum-structure" id="toc-curriculum-structure" class="nav-link active" data-scroll-target="#curriculum-structure">Curriculum structure</a></li>
  <li><a href="#part-i-meaning-and-orientation" id="toc-part-i-meaning-and-orientation" class="nav-link" data-scroll-target="#part-i-meaning-and-orientation">Part I: Meaning and Orientation</a>
  <ul class="collapse">
  <li><a href="#week-1-jan-2023-framing-interpretability" id="toc-week-1-jan-2023-framing-interpretability" class="nav-link" data-scroll-target="#week-1-jan-2023-framing-interpretability">Week 1 (Jan 20–23) — Framing Interpretability</a></li>
  <li><a href="#week-2-jan-2730-mechanistic-interpretability-and-circuits" id="toc-week-2-jan-2730-mechanistic-interpretability-and-circuits" class="nav-link" data-scroll-target="#week-2-jan-2730-mechanistic-interpretability-and-circuits">Week 2 (Jan 27–30) — Mechanistic Interpretability and Circuits</a></li>
  <li><a href="#week-3-feb-36-interpretability-as-a-safety-strategy" id="toc-week-3-feb-36-interpretability-as-a-safety-strategy" class="nav-link" data-scroll-target="#week-3-feb-36-interpretability-as-a-safety-strategy">Week 3 (Feb 3–6) — Interpretability as a Safety Strategy</a></li>
  </ul></li>
  <li><a href="#part-ii-methods-interventions-and-internal-structure" id="toc-part-ii-methods-interventions-and-internal-structure" class="nav-link" data-scroll-target="#part-ii-methods-interventions-and-internal-structure">Part II: Methods, Interventions, and Internal Structure</a>
  <ul class="collapse">
  <li><a href="#week-4-feb-1013-causal-intervention-in-practice" id="toc-week-4-feb-1013-causal-intervention-in-practice" class="nav-link" data-scroll-target="#week-4-feb-1013-causal-intervention-in-practice">Week 4 (Feb 10–13) — Causal Intervention in Practice</a></li>
  <li><a href="#week-5-feb-1720-steering-vectors" id="toc-week-5-feb-1720-steering-vectors" class="nav-link" data-scroll-target="#week-5-feb-1720-steering-vectors">Week 5 (Feb 17–20) — Steering Vectors</a></li>
  <li><a href="#week-6-feb-2427-behavioral-directions-and-internal-state-variables" id="toc-week-6-feb-2427-behavioral-directions-and-internal-state-variables" class="nav-link" data-scroll-target="#week-6-feb-2427-behavioral-directions-and-internal-state-variables">Week 6 (Feb 24–27) — Behavioral Directions and Internal State Variables</a></li>
  <li><a href="#week-7-mar-36-machine-unlearning" id="toc-week-7-mar-36-machine-unlearning" class="nav-link" data-scroll-target="#week-7-mar-36-machine-unlearning">Week 7 (Mar 3–6) — Machine Unlearning</a></li>
  <li><a href="#week-8-mar-1013-latent-knowledge" id="toc-week-8-mar-1013-latent-knowledge" class="nav-link" data-scroll-target="#week-8-mar-1013-latent-knowledge">Week 8 (Mar 10–13) — Latent Knowledge</a></li>
  <li><a href="#week-9-mar-1720-causal-scrubbing" id="toc-week-9-mar-1720-causal-scrubbing" class="nav-link" data-scroll-target="#week-9-mar-1720-causal-scrubbing">Week 9 (Mar 17–20) — Causal Scrubbing</a></li>
  <li><a href="#week-10-mar-31apr-3-large-scale-mechanistic-interpretability" id="toc-week-10-mar-31apr-3-large-scale-mechanistic-interpretability" class="nav-link" data-scroll-target="#week-10-mar-31apr-3-large-scale-mechanistic-interpretability">Week 10 (Mar 31–Apr 3) — Large-Scale Mechanistic Interpretability</a></li>
  </ul></li>
  <li><a href="#part-iii-limits-evaluation-and-stakes" id="toc-part-iii-limits-evaluation-and-stakes" class="nav-link" data-scroll-target="#part-iii-limits-evaluation-and-stakes">Part III: Limits, Evaluation, and Stakes</a>
  <ul class="collapse">
  <li><a href="#week-11-apr-710-mesa-optimization-and-deceptive-alignment" id="toc-week-11-apr-710-mesa-optimization-and-deceptive-alignment" class="nav-link" data-scroll-target="#week-11-apr-710-mesa-optimization-and-deceptive-alignment">Week 11 (Apr 7–10) — Mesa-Optimization and Deceptive Alignment</a></li>
  <li><a href="#week-12-apr-1417-deceptive-interpretability" id="toc-week-12-apr-1417-deceptive-interpretability" class="nav-link" data-scroll-target="#week-12-apr-1417-deceptive-interpretability">Week 12 (Apr 14–17) — Deceptive Interpretability</a></li>
  <li><a href="#week-13-apr-2124-philosophy-of-causal-explanation" id="toc-week-13-apr-2124-philosophy-of-causal-explanation" class="nav-link" data-scroll-target="#week-13-apr-2124-philosophy-of-causal-explanation">Week 13 (Apr 21–24) — Philosophy of Causal Explanation</a></li>
  <li><a href="#week-14-apr-28may-1-causal-abstractions" id="toc-week-14-apr-28may-1-causal-abstractions" class="nav-link" data-scroll-target="#week-14-apr-28may-1-causal-abstractions">Week 14 (Apr 28–May 1) — Causal Abstractions</a></li>
  <li><a href="#week-15-may-58-interpretability-and-existential-risk" id="toc-week-15-may-58-interpretability-and-existential-risk" class="nav-link" data-scroll-target="#week-15-may-58-interpretability-and-existential-risk">Week 15 (May 5–8) — Interpretability and Existential Risk</a></li>
  </ul></li>
  <li><a href="#expectations-and-participation" id="toc-expectations-and-participation" class="nav-link" data-scroll-target="#expectations-and-participation">Expectations and Participation</a>
  <ul class="collapse">
  <li><a href="#preparation" id="toc-preparation" class="nav-link" data-scroll-target="#preparation">Preparation</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">AI Interpretability: Meaning, Methods, and Limitations</h1>
<p class="subtitle lead">Graduate Reading Group (UC Berkeley, Spring 2026)</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled" title="Interested?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interested?
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://forms.gle/WBqohweBJutZSQ7b7">RSVP for this reading group</a> to indicate your interest and availability.</li>
<li><a href="https://berkeley-ai-risk.github.io">Discover other programming</a> from Berkeley’s AI Risk group, or <a href="https://forms.gle/LnzZKT5FR5c7nGKc8">join the mailing list</a></li>
</ul>
</div>
</div>
<section id="curriculum-structure" class="level2">
<h2 class="anchored" data-anchor-id="curriculum-structure">Curriculum structure</h2>
<p>The course follows the arc of its title:</p>
<ul>
<li><strong>Part I: Meaning and Orientation</strong> (Weeks 1–3) — What is interpretability for? What kinds of questions can it answer?</li>
<li><strong>Part II: Methods, Interventions, and Internal Structure</strong> (Weeks 4–10) — What can we actually do with real models, and what kinds of causal claims follow?</li>
<li><strong>Part III: Limits, Evaluation, and Stakes</strong> (Weeks 11–15) — When does interpretability work, and what role should it play in mitigating AI risk?</li>
</ul>
<p>The schedule below is tentative. Please do not hesitate to contact Will Fithian (<a href="mailto:wfithian@berkeley.edu">wfithian@berkeley.edu</a>) if you have additional papers or topics to suggest.</p>
<hr>
</section>
<section id="part-i-meaning-and-orientation" class="level2">
<h2 class="anchored" data-anchor-id="part-i-meaning-and-orientation">Part I: Meaning and Orientation</h2>
<section id="week-1-jan-2023-framing-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="week-1-jan-2023-framing-interpretability">Week 1 (Jan 20–23) — Framing Interpretability</h3>
<dl>
<dt><strong>Towards a Rigorous Science of Interpretable Machine Learning</strong> (2017)</dt>
<dd>
Finale Doshi-Velez &amp; Been Kim
</dd>
<dd>
<a href="https://arxiv.org/abs/1702.08608" class="uri">https://arxiv.org/abs/1702.08608</a>
</dd>
</dl>
<p><em>Introduces a taxonomy of interpretability goals, audiences, and evaluation criteria; frames interpretability as an epistemic problem.</em></p>
</section>
<section id="week-2-jan-2730-mechanistic-interpretability-and-circuits" class="level3">
<h3 class="anchored" data-anchor-id="week-2-jan-2730-mechanistic-interpretability-and-circuits">Week 2 (Jan 27–30) — Mechanistic Interpretability and Circuits</h3>
<dl>
<dt><strong>Zoom In: Circuits</strong> (2020)</dt>
<dd>
Chris Olah et al.
</dd>
<dd>
<a href="https://distill.pub/2020/circuits/" class="uri">https://distill.pub/2020/circuits/</a>
</dd>
</dl>
<p><em>Canonical introduction to mechanistic interpretability, circuit-based explanations, and feature-level analysis.</em></p>
</section>
<section id="week-3-feb-36-interpretability-as-a-safety-strategy" class="level3">
<h3 class="anchored" data-anchor-id="week-3-feb-36-interpretability-as-a-safety-strategy">Week 3 (Feb 3–6) — Interpretability as a Safety Strategy</h3>
<p><strong>Neel Nanda</strong> (2025)</p>
<ul>
<li><em>A Pragmatic Vision for Interpretability</em> <a href="https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability" class="uri">https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability</a></li>
<li><em>How Can Interpretability Researchers Help AGI Go Well?</em> <a href="https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well" class="uri">https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well</a></li>
</ul>
<p><em>Agenda-setting essays that articulate a pragmatic, safety-oriented theory of change for interpretability research.</em></p>
<hr>
</section>
</section>
<section id="part-ii-methods-interventions-and-internal-structure" class="level2">
<h2 class="anchored" data-anchor-id="part-ii-methods-interventions-and-internal-structure">Part II: Methods, Interventions, and Internal Structure</h2>
<section id="week-4-feb-1013-causal-intervention-in-practice" class="level3">
<h3 class="anchored" data-anchor-id="week-4-feb-1013-causal-intervention-in-practice">Week 4 (Feb 10–13) — Causal Intervention in Practice</h3>
<dl>
<dt><strong>Locating and Editing Factual Associations in GPT</strong> (NeurIPS 2022)</dt>
<dd>
Kevin Meng et al.
</dd>
<dd>
<a href="https://arxiv.org/abs/2202.05262" class="uri">https://arxiv.org/abs/2202.05262</a>
</dd>
<dd>
Project page: <a href="https://rome.baulab.info/" class="uri">https://rome.baulab.info/</a>
</dd>
</dl>
<p><em>ROME: causal tracing to identify where factual associations live in transformer computation, and targeted weight editing to modify them.</em></p>
</section>
<section id="week-5-feb-1720-steering-vectors" class="level3">
<h3 class="anchored" data-anchor-id="week-5-feb-1720-steering-vectors">Week 5 (Feb 17–20) — Steering Vectors</h3>
<dl>
<dt><strong>Steering Language Models With Activation Engineering</strong> (2023)</dt>
<dd>
Alexander Matt Turner et al.
</dd>
<dd>
<a href="https://arxiv.org/abs/2308.10248" class="uri">https://arxiv.org/abs/2308.10248</a>
</dd>
</dl>
<p><em>Activation Addition (ActAdd): inference-time steering by adding activation-space directions computed from prompt pairs; foundational steering method.</em></p>
</section>
<section id="week-6-feb-2427-behavioral-directions-and-internal-state-variables" class="level3">
<h3 class="anchored" data-anchor-id="week-6-feb-2427-behavioral-directions-and-internal-state-variables">Week 6 (Feb 24–27) — Behavioral Directions and Internal State Variables</h3>
<dl>
<dt><strong>Refusal in Language Models Is Mediated by a Single Direction</strong> (2024)</dt>
<dd>
Yoni Arditi, Neel Nanda et al.
</dd>
<dd>
<a href="https://arxiv.org/abs/2402.07896" class="uri">https://arxiv.org/abs/2402.07896</a>
</dd>
<dt><strong>Persona Vectors: Monitoring and Steering Model Behavior</strong> (2025)</dt>
<dd>
Anthropic Interpretability Team
</dd>
<dd>
<a href="https://www.anthropic.com/research/persona-vectors" class="uri">https://www.anthropic.com/research/persona-vectors</a>
</dd>
</dl>
<p><em>Together, these readings show that important model behaviors can be mediated by low-dimensional structure in activation space. The refusal paper provides a crisp mechanistic result for a single high-stakes behavior, while persona vectors generalize this idea to broader, more persistent behavioral traits and recast interpretability as monitoring and oversight infrastructure. This week sets up later discussions about robustness, abstraction, and the limits of behavioral interpretability.</em></p>
</section>
<section id="week-7-mar-36-machine-unlearning" class="level3">
<h3 class="anchored" data-anchor-id="week-7-mar-36-machine-unlearning">Week 7 (Mar 3–6) — Machine Unlearning</h3>
<dl>
<dt><strong>TOFU: A Task of Fictitious Unlearning for LLMs</strong> (2024)</dt>
<dd>
Pratyush Maini et al.
</dd>
<dd>
<a href="https://arxiv.org/abs/2401.06121" class="uri">https://arxiv.org/abs/2401.06121</a>
</dd>
</dl>
<p><em>Benchmark and evaluation suite for LLM unlearning; clarifies what it would mean to behave as if certain data were never learned.</em></p>
</section>
<section id="week-8-mar-1013-latent-knowledge" class="level3">
<h3 class="anchored" data-anchor-id="week-8-mar-1013-latent-knowledge">Week 8 (Mar 10–13) — Latent Knowledge</h3>
<dl>
<dt><strong>Discovering Latent Knowledge in Language Models Without Supervision</strong> (ICLR 2023; arXiv 2022)</dt>
<dd>
Collin Burns et al.
</dd>
<dd>
<a href="https://arxiv.org/abs/2212.03827" class="uri">https://arxiv.org/abs/2212.03827</a>
</dd>
</dl>
<p><em>Extracts truth-related structure from activations to separate what models know from what they say; foundational for oversight.</em></p>
</section>
<section id="week-9-mar-1720-causal-scrubbing" class="level3">
<h3 class="anchored" data-anchor-id="week-9-mar-1720-causal-scrubbing">Week 9 (Mar 17–20) — Causal Scrubbing</h3>
<dl>
<dt><strong>Causal Scrubbing: A Method for Rigorously Testing Mechanistic Interpretability Hypotheses</strong> (2022)</dt>
<dd>
Lawrence Chan et al.
</dd>
<dd>
<a href="https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing" class="uri">https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing</a>
</dd>
</dl>
<p><em>Behavior-preserving resampling ablations as a principled way to test mechanistic hypotheses (interpretation as a falsifiable causal claim).</em></p>
<p><em>Spring recess: Mar 23–27</em></p>
</section>
<section id="week-10-mar-31apr-3-large-scale-mechanistic-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="week-10-mar-31apr-3-large-scale-mechanistic-interpretability">Week 10 (Mar 31–Apr 3) — Large-Scale Mechanistic Interpretability</h3>
<dl>
<dt><strong>On the Biology of a Large Language Model</strong> (2025)</dt>
<dd>
Anthropic Interpretability Team
</dd>
<dd>
<a href="https://www.anthropic.com/research/biology-of-a-large-language-model" class="uri">https://www.anthropic.com/research/biology-of-a-large-language-model</a>
</dd>
</dl>
<p><em>Anthropic’s most ambitious attempt to construct detailed mechanistic explanations of a frontier language model, tracing behavior through features, circuits, and attribution graphs.</em></p>
<hr>
</section>
</section>
<section id="part-iii-limits-evaluation-and-stakes" class="level2">
<h2 class="anchored" data-anchor-id="part-iii-limits-evaluation-and-stakes">Part III: Limits, Evaluation, and Stakes</h2>
<section id="week-11-apr-710-mesa-optimization-and-deceptive-alignment" class="level3">
<h3 class="anchored" data-anchor-id="week-11-apr-710-mesa-optimization-and-deceptive-alignment">Week 11 (Apr 7–10) — Mesa-Optimization and Deceptive Alignment</h3>
<dl>
<dt><strong>Risks from Learned Optimization in Advanced ML Systems</strong> (2019)</dt>
<dd>
Evan Hubinger et al.
</dd>
<dd>
<a href="https://arxiv.org/abs/1906.01820" class="uri">https://arxiv.org/abs/1906.01820</a>
</dd>
</dl>
<p><em>Classic account of mesa-optimization and deceptive alignment; essential for understanding interpretability’s limits.</em></p>
</section>
<section id="week-12-apr-1417-deceptive-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="week-12-apr-1417-deceptive-interpretability">Week 12 (Apr 14–17) — Deceptive Interpretability</h3>
<dl>
<dt><strong>Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems</strong> (2025)</dt>
<dd>
Simon Lermen, Mateusz Dziemian, Natalia Pérez-Campanero Antolín
</dd>
<dd>
<a href="https://arxiv.org/abs/2504.07831" class="uri">https://arxiv.org/abs/2504.07831</a>
</dd>
</dl>
<p><em>Shows how models/agents can produce deceptive explanations that evade automated interpretability-based oversight; stress-tests interpretability as safety infrastructure.</em></p>
</section>
<section id="week-13-apr-2124-philosophy-of-causal-explanation" class="level3">
<h3 class="anchored" data-anchor-id="week-13-apr-2124-philosophy-of-causal-explanation">Week 13 (Apr 21–24) — Philosophy of Causal Explanation</h3>
<dl>
<dt><strong>Making Things Happen</strong> (2003), selected chapters</dt>
<dd>
James Woodward
</dd>
</dl>
<p><em>Philosophical grounding for causal explanation and intervention.</em></p>
</section>
<section id="week-14-apr-28may-1-causal-abstractions" class="level3">
<h3 class="anchored" data-anchor-id="week-14-apr-28may-1-causal-abstractions">Week 14 (Apr 28–May 1) — Causal Abstractions</h3>
<dl>
<dt><strong>Causal Abstractions of Neural Networks</strong> (2023)</dt>
<dd>
Atticus Geiger, DeepMind et al.
</dd>
<dd>
<a href="https://arxiv.org/abs/2303.02536" class="uri">https://arxiv.org/abs/2303.02536</a>
</dd>
</dl>
<p><em>Asks whether mechanistic interpretability explanations support valid higher-level causal abstractions. Develops formal criteria for abstraction-preserving interventions and shows that many mechanistic stories fail to license the explanations they appear to offer.</em></p>
</section>
<section id="week-15-may-58-interpretability-and-existential-risk" class="level3">
<h3 class="anchored" data-anchor-id="week-15-may-58-interpretability-and-existential-risk">Week 15 (May 5–8) — Interpretability and Existential Risk</h3>
<dl>
<dt><strong>Is Power-Seeking AI an Existential Risk?</strong> (2022–2024), selected sections</dt>
<dd>
Joseph Carlsmith
</dd>
<dd>
<a href="https://arxiv.org/abs/2206.13353" class="uri">https://arxiv.org/abs/2206.13353</a>
</dd>
</dl>
<p><em>Situates interpretability within alignment, governance, and long-term AI risk.</em></p>
<hr>
</section>
</section>
<section id="expectations-and-participation" class="level2">
<h2 class="anchored" data-anchor-id="expectations-and-participation">Expectations and Participation</h2>
<section id="preparation" class="level3">
<h3 class="anchored" data-anchor-id="preparation">Preparation</h3>
<ul>
<li>Complete the assigned reading before each session</li>
<li>Come prepared with questions, critiques, or connections to other work</li>
</ul>
</section>
<section id="discussion" class="level3">
<h3 class="anchored" data-anchor-id="discussion">Discussion</h3>
<ul>
<li>Active participation is expected; the value of the group depends on collective engagement</li>
<li>Discussions will focus on: What does this paper claim? How do we evaluate those claims? What are the implications for AI safety?</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/wfithian\.github\.io\/ai-interpretability\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>