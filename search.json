[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "AI Interpretability: Meaning, Methods, and Limitations",
    "section": "",
    "text": "Interested?\n\n\n\n\nRSVP for this reading group to indicate your interest and availability.\nDiscover other programming from Berkeley’s AI Risk group, or join the mailing list"
  },
  {
    "objectID": "syllabus.html#curriculum-structure",
    "href": "syllabus.html#curriculum-structure",
    "title": "AI Interpretability: Meaning, Methods, and Limitations",
    "section": "Curriculum structure",
    "text": "Curriculum structure\nThe course follows the arc of its title:\n\nPart I: Meaning (Weeks 1–4) — What is interpretability for? What kinds of questions can it answer?\nPart II: Methods (Weeks 5–13) — What can we actually do with real models, and what kinds of causal claims follow?\nPart III: Limits (Weeks 14–15) — When does interpretability work, and what role should it play in mitigating AI risk?\n\nThe schedule below is tentative. Please do not hesitate to contact Will Fithian (wfithian@berkeley.edu) if you have additional papers or topics to suggest."
  },
  {
    "objectID": "syllabus.html#part-i-meaning",
    "href": "syllabus.html#part-i-meaning",
    "title": "AI Interpretability: Meaning, Methods, and Limitations",
    "section": "Part I: Meaning",
    "text": "Part I: Meaning\n\nWeek 1 (Jan 20–23) — Framing Interpretability\n\nTowards a Rigorous Science of Interpretable Machine Learning (2017)\n\nFinale Doshi-Velez & Been Kim\n\n\nhttps://arxiv.org/abs/1702.08608\n\n\nIntroduces a taxonomy of interpretability goals, audiences, and evaluation criteria; frames interpretability as an epistemic problem.\n\n\nWeek 2 (Jan 27–30) — Interpretability in Neuroscience\n\nCrafting Interpretable Embeddings for Language Neuroscience by Asking LLMs Questions (2025)\n\nKushal Benara et al.\n\n\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC12021422/\n\n\nUses LLM-generated questions to build interpretable feature spaces for predicting brain activity from language stimuli.\n\nPredictive Coding or Just Feature Discovery? An Alternative Account of Why Language Models Fit Brain Data (2025)\n\nRichard Antonello & Alexander Huth\n\n\nhttps://direct.mit.edu/nol/article/5/1/64/113632/Predictive-Coding-or-Just-Feature-Discovery-An\n\n\nChallenges the interpretation that LLM–brain alignment reflects predictive coding, arguing it may reflect shared feature discovery instead.\n\n\nWeek 3 (Feb 3–6) — Mechanistic Interpretability and Circuits\n\nZoom In: Circuits (2020)\n\nChris Olah et al.\n\n\nhttps://distill.pub/2020/circuits/\n\n\nCanonical introduction to mechanistic interpretability, circuit-based explanations, and feature-level analysis.\n\n\nWeek 4 (Feb 10–13) — Interpretability as a Safety Strategy\nNeel Nanda (2025)\n\nA Pragmatic Vision for Interpretability https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability\nHow Can Interpretability Researchers Help AGI Go Well? https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well\n\nAgenda-setting essays that articulate a pragmatic, safety-oriented theory of change for interpretability research."
  },
  {
    "objectID": "syllabus.html#part-ii-methods",
    "href": "syllabus.html#part-ii-methods",
    "title": "AI Interpretability: Meaning, Methods, and Limitations",
    "section": "Part II: Methods",
    "text": "Part II: Methods\n\nWeek 5 (Feb 17–20) — Causal Intervention in Practice\n\nLocating and Editing Factual Associations in GPT (NeurIPS 2022)\n\nKevin Meng et al.\n\n\nhttps://arxiv.org/abs/2202.05262\n\n\nProject page: https://rome.baulab.info/\n\n\nROME: causal tracing to identify where factual associations live in transformer computation, and targeted weight editing to modify them.\n\nHow to Use and Interpret Activation Patching (2024)\n\nStefan Heimersheim & Neel Nanda\n\n\nhttps://arxiv.org/abs/2404.15255\n\n\nPractical guide to activation patching methodology, clarifying when and how causal interventions yield meaningful interpretability claims.\n\n\nWeek 6 (Feb 24–27) — Steering Vectors\n\nSteering Language Models With Activation Engineering (2023)\n\nAlexander Matt Turner et al.\n\n\nhttps://arxiv.org/abs/2308.10248\n\n\nActivation Addition (ActAdd): inference-time steering by adding activation-space directions computed from prompt pairs; foundational steering method.\n\n\nWeek 7 (Mar 3–6) — Behavioral Directions and Internal State Variables\n\nRefusal in Language Models Is Mediated by a Single Direction (2024)\n\nYoni Arditi, Neel Nanda et al.\n\n\nhttps://arxiv.org/abs/2402.07896\n\nPersona Vectors: Monitoring and Steering Model Behavior (2025)\n\nAnthropic Interpretability Team\n\n\nhttps://www.anthropic.com/research/persona-vectors\n\n\nTogether, these readings show that important model behaviors can be mediated by low-dimensional structure in activation space. The refusal paper provides a crisp mechanistic result for a single high-stakes behavior, while persona vectors generalize this idea to broader, more persistent behavioral traits and recast interpretability as monitoring and oversight infrastructure. This week sets up later discussions about robustness, abstraction, and the limits of behavioral interpretability.\n\n\nWeek 8 (Mar 10–13) — Machine Unlearning\n\nTOFU: A Task of Fictitious Unlearning for LLMs (2024)\n\nPratyush Maini et al.\n\n\nhttps://arxiv.org/abs/2401.06121\n\n\nBenchmark and evaluation suite for LLM unlearning; clarifies what it would mean to behave as if certain data were never learned.\n\n\nWeek 9 (Mar 17–20) — Latent Knowledge\n\nDiscovering Latent Knowledge in Language Models Without Supervision (ICLR 2023; arXiv 2022)\n\nCollin Burns et al.\n\n\nhttps://arxiv.org/abs/2212.03827\n\n\nExtracts truth-related structure from activations to separate what models know from what they say; foundational for oversight.\nSpring recess: Mar 23–27\n\n\nWeek 10 (Mar 31–Apr 3) — Causal Scrubbing\n\nCausal Scrubbing: A Method for Rigorously Testing Mechanistic Interpretability Hypotheses (2022)\n\nLawrence Chan et al.\n\n\nhttps://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing\n\n\nBehavior-preserving resampling ablations as a principled way to test mechanistic hypotheses (interpretation as a falsifiable causal claim).\n\n\nWeek 11 (Apr 7–10) — Causal Abstractions\n\nCausal Abstraction: A Theoretical Foundation for Mechanistic Interpretability (2023–2025)\n\nAtticus Geiger et al.\n\n\nhttps://arxiv.org/abs/2301.04709\n\n\nDevelops formal criteria for when mechanistic explanations support valid higher-level causal abstractions, grounding interpretability claims in intervention semantics.\n\n\nWeek 12 (Apr 14–17) — Sparse Auto-encoders\n\nTowards Monosemanticity: Decomposing Language Models With Dictionary Learning (2023)\n\nAnthropic Interpretability Team\n\n\nhttps://transformer-circuits.pub/2023/monosemantic-features\n\n\nUses sparse dictionary learning to decompose MLP activations into interpretable, monosemantic features; foundational method for scalable mechanistic interpretability.\n\n\nWeek 13 (Apr 21–24) — Large-Scale Mechanistic Interpretability\n\nOn the Biology of a Large Language Model (2025)\n\nAnthropic Interpretability Team\n\n\nhttps://www.anthropic.com/research/biology-of-a-large-language-model\n\n\nAnthropic’s most ambitious attempt to construct detailed mechanistic explanations of a frontier language model, tracing behavior through features, circuits, and attribution graphs."
  },
  {
    "objectID": "syllabus.html#part-iii-limits",
    "href": "syllabus.html#part-iii-limits",
    "title": "AI Interpretability: Meaning, Methods, and Limitations",
    "section": "Part III: Limits",
    "text": "Part III: Limits\n\nWeek 14 (Apr 28–May 1) — Mesa-Optimization and Deceptive Alignment\n\nRisks from Learned Optimization in Advanced ML Systems (2019)\n\nEvan Hubinger et al.\n\n\nhttps://arxiv.org/abs/1906.01820\n\n\nClassic account of mesa-optimization and deceptive alignment; essential for understanding interpretability’s limits.\n\n\nWeek 15 (May 5–8) — Deceptive Interpretability\n\nDeceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems (2025)\n\nSimon Lermen, Mateusz Dziemian, Natalia Pérez-Campanero Antolín\n\n\nhttps://arxiv.org/abs/2504.07831\n\n\nShows how models/agents can produce deceptive explanations that evade automated interpretability-based oversight; stress-tests interpretability as safety infrastructure."
  },
  {
    "objectID": "syllabus.html#expectations-and-participation",
    "href": "syllabus.html#expectations-and-participation",
    "title": "AI Interpretability: Meaning, Methods, and Limitations",
    "section": "Expectations and Participation",
    "text": "Expectations and Participation\n\nPreparation\n\nComplete the assigned reading before each session\nCome prepared with questions, critiques, or connections to other work\n\n\n\nDiscussion\n\nActive participation is expected; the value of the group depends on collective engagement\nDiscussions will focus on: What does this paper claim? How do we evaluate those claims? What are the implications for AI safety?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI Interpretability: Meaning, Methods, and Limitations",
    "section": "",
    "text": "Interested?\n\n\n\n\nRSVP for this reading group to indicate your interest and availability.\nDiscover other programming from Berkeley’s AI Risk group, or join the mailing list"
  },
  {
    "objectID": "index.html#logistics",
    "href": "index.html#logistics",
    "title": "AI Interpretability: Meaning, Methods, and Limitations",
    "section": "Logistics",
    "text": "Logistics\nFormat: We will read 1–2 papers per week; one group member will lead a 45-minute presentation followed by group discussion.\nOrganizers: Will Fithian (Statistics), Wes Holliday (Philosophy), and Sean Richardson (Statistics PhD student)\nMeeting Details: TBD; please indicate your availability via the RSVP form.\nCourse credit: We can enroll a limited number of students for up to 2 units of course credit. The requirement for course credit is regular attendance plus presenting a paper at one session. Please indicate your interest via the RSVP form."
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "AI Interpretability: Meaning, Methods, and Limitations",
    "section": "Description",
    "text": "Description\nThis is an interdisciplinary reading group organized around the question:\n\nWhat kinds of interpretability claims are meaningful and reliable as AI systems become more capable and are deployed in higher-stakes contexts?\n\nWe will study interpretability as a scientific and safety-relevant practice: a collection of methods, epistemic standards, and theory-of-change assumptions aimed at gaining traction on the internal structure of modern ML systems in ways that could plausibly matter for oversight, control, and alignment. Questions include:\n\nWhy study interpretability? How do we formulate interpretability claims rigorously, and assess evidence for and against?\nHow are interpretability methods currently used to understand and steer state-of-the-art systems? How do we know if they are working?\nWhat are the limitations of these methods, or of the interpretability paradigm more broadly? What role can it play in reducing risks to society?\n\nOur aim is to bring together an interdisciplinary community including perspectives from machine learning, statistics, causal inference, philosophy of science, and AI governance, and to seed new research programs.\n\nLearning objectives\nBy the end of this course, participants should be able to:\n\nDistinguish different senses of “interpretability” and “understanding”\nEvaluate mechanistic claims in light of causal and robustness criteria\nRecognize when interpretability evidence is strong, weak, or misleading\nArticulate realistic theories of change connecting interpretability to AI risk mitigation\n\n\n\nAssumed background\nWe welcome broad interdisciplinary participation, but our curriculum is designed for participants with:\n\nComfort reading technical ML papers\nInterest in evaluating interpretability claims critically\nBackground in one or more relevant fields such as statistics, philosophy, causal inference, or cognitive science\n\nNo prior interpretability research experience is required."
  },
  {
    "objectID": "index.html#tentative-reading-schedule",
    "href": "index.html#tentative-reading-schedule",
    "title": "AI Interpretability: Meaning, Methods, and Limitations",
    "section": "Tentative Reading Schedule",
    "text": "Tentative Reading Schedule\nThe reading list below may be revised according to participant interests.\n\nPart I: Meaning (Weeks 1–4)\nWhat is interpretability for? What kinds of questions can it answer? How do mechanistic explanations differ from post-hoc rationalizations?\n\n\n\nWeek\nDates\nTopic\nReading\n\n\n\n\n1\nJan 20–23\nFraming Interpretability\nTowards a Rigorous Science of Interpretable Machine Learning — Doshi-Velez & Kim (2017)\n\n\n2\nJan 27–30\nInterpretability in Neuroscience\nCrafting Interpretable Embeddings for Language Neuroscience by Asking LLMs Questions — Benara et al. (2025) + Predictive Coding or Just Feature Discovery? — Antonello & Huth (2025)\n\n\n3\nFeb 3–6\nMechanistic Interpretability\nZoom In: Circuits — Olah et al. (2020)\n\n\n4\nFeb 10–13\nInterpretability as Safety Strategy\nA Pragmatic Vision for Interpretability + How Can Interpretability Researchers Help AGI Go Well? — Nanda (2025)\n\n\n\n\n\nPart II: Methods (Weeks 5–13)\nWhat can interpretability methods actually do with real models? When does an intervention support an explanatory claim, and when is it merely a control knob?\n\n\n\nWeek\nDates\nTopic\nReading\n\n\n\n\n5\nFeb 17–20\nCausal Intervention (ROME)\nLocating and Editing Factual Associations in GPT — Meng et al. (NeurIPS 2022) + How to use and interpret activation patching — Heimersheim & Nanda (2024)\n\n\n6\nFeb 24–27\nSteering Vectors\nSteering Language Models With Activation Engineering — Turner et al. (2023)\n\n\n7\nMar 3–6\nBehavioral Directions\nRefusal in LLMs Is Mediated by a Single Direction — Arditi et al. (2024) + Persona Vectors — Anthropic (2025)\n\n\n8\nMar 10–13\nMachine Unlearning\nTOFU: A Task of Fictitious Unlearning for LLMs — Maini et al. (2024)\n\n\n9\nMar 17–20\nLatent Knowledge\nDiscovering Latent Knowledge in Language Models Without Supervision — Burns et al. (ICLR 2023)\n\n\n\nMar 23–27\nSpring recess\n\n\n\n10\nMar 31–Apr 3\nCausal Scrubbing\nCausal Scrubbing: Rigorously Testing Mechanistic Hypotheses — Chan et al. (2022)\n\n\n11\nApr 7–10\nCausal Abstractions\nCausal Abstraction: A Theoretical Foundation for Mechanistic Interpretability — Geiger et al. (2023–2025)\n\n\n12\nApr 14–17\nSparse Auto-encoders\nTowards Monosemanticity: Decomposing Language Models With Dictionary Learning\n\n\n13\nApr 21–24\nLarge-Scale Mechanistic Interp\nOn the Biology of a Large Language Model — Anthropic (2025)\n\n\n\n\n\nPart III: Limits (Weeks 14–15)\nWhen does interpretability evidence support counterfactual reasoning? Given its limits, what role can interpretability play in reducing catastrophic risk?\n\n\n\nWeek\nDates\nTopic\nReading\n\n\n\n\n14\nApr 28–May 1\nMesa-Optimization\nRisks from Learned Optimization in Advanced ML Systems — Hubinger et al. (2019)\n\n\n15\nMay 5–8\nDeceptive Interpretability\nDeceptive Automated Interpretability: LMs Coordinating to Fool Oversight — Lermen et al. (2025)\n\n\n\nSee the full syllabus for paper summaries and additional context."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "AI Interpretability: Meaning, Methods, and Limitations",
    "section": "Contact",
    "text": "Contact\nQuestions? Contact Will Fithian (wfithian@berkeley.edu)."
  }
]